---
layout:     post
title:      Chapter 9. Volumes
subtitle:   attaching disk storage to containers
date:       2020-09-27
author:     Hao
header-img: img/post/post_bg_coffee.jpg
catalog: true
mathjax: true
tags:
    - Kubernetes
---

在之前的几章中，我们学习了 Pod，Controllers 和 Service。这一章我们继续回到 Pod 内部，学习它的 containers 如何访问和互相共享外部存储。

我们可以把 Pod 看作一个逻辑上的 host，运行在其中的进程（即 containers）共享 CPU、RAM、网络接口等其他资源。但文件系统并非如此，Pod 中的每个 container 都有自己的文件系统，因为 containers 的文件系统来自于构建它的 image。所以即便是运行在相同 Pod 的 containers，它们之间也不能互相看到对方的文件系统。

但在某种场景下，我们想让新创建的 container 可以从上一个完成的地方继续运行。Kubernetes 通过定义了 *Volumens* 来提供这项功能。Volumes 不是 Kubernetes 中最上层的资源对象，它可以被认为是 Pod 的一部分，依附于 Pod 存在，与 Pod 有相同的生命周期。这就意味着，Pod 创建时 Volume 也被创建，Pod 销毁时 Volume 也被销毁。因此，Volume 中的内容可以在多个 containers 的生命周期中存在。一个新创建的 container 可以看到上一个 container 写入该 Volume 的文件。

### Introducing volumes

作为 Pod 的一部分，Volume 不能被单独创建，它只能在 Pod 的描述文件中被创建，这有点像 container。一个 Volume 只要被这个 Pod 中的 containers 挂载，它就可以被这些挂载的 containers 访问到。

#### Explaining Volumes with an exmaple

举个例子来解释 Volume。假设我们有一个 Pod，其中运行着三个 containers，如下图所示。一个 ContentAgent 容器用来创建 HTML 页面，并把它保存在 */var/html* 文件夹；一个 WebServer 容器用来将 *var/htdocs* 文件夹下的 HTML 页面生成 web 服务器，并把访问的日志保存在 */var/logs* 文件夹。还有一个 logRotator 容器用来分析处理 */var/logs* 目录下的日志。

![img](/img/post/post_volume_exm.png)

每个 container 都有简单清楚的任务。正常情况下，这样的 Pod 没有任何意义，因为 ContentAgent 容器创建的 HTML 页面，WebServer 容器访问不到；WebServer 容器保存的访问日志 logRotator 容器也访问不到。

但当我们给这个 Pod 添加两个 Volumes，并把它们合适地挂载在三个 containers 上，三个容器就可以一起工作。首先，我们创建一个 publicHtml Volume，它被同时挂载在 ContentAgent 容器的 */var/html* 和 WebServer 容器的 *var/htdocs* 目录。WebServer 就可以访问到 ContentAgent 容器生成的 HTML 页面了。

同样地，创建一个 logVol Volume，挂载在 WEbServer 容器的 */var/logs* 目录和 logRotator 容器的 */var/logs* 目录，这样 logRorator 容器就可以访问到 WebServer 生成的访问日志了。

#### Introducing Volume types

官方提供了很多种 Volume 类型。一些是常用的，其他是针对于特定的存储技术。在上面的例子中，初始情况下两个 Volume 都是空的，所以可以使用 emptyDir 类型的 Volume。一共有以下类型：

+ emptyDir：空目录，用于存放临时数据；
+ hostPath：把 Worker Node 的文件系统的目录挂载到 Pod 中；
+ gitRepo：通过 Git 仓库内容来初始化的 Volume；
+ nfs：An NFS share mounted into the pod；
+ gcePersistentDisk(Google Compute Engine), awsElasticBlockStore(AWS), azureDisk(Azure)：挂载云提供商的特定存储；
+ cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphere- Volume, photonPersistentDisk, scaleIO：挂载其他类型的网络存储；
+ configMap, secret, sownloadAPI：给 Pod 暴露某些资源信息的特定类型；
+ persistentVolumeClaim：A way to use a pre- or dynamically provisioned per- sistent storage；

有很多类型笔者也不是很清楚。这些类型都是服务于不同的目的。一个 Pod 可以同时有多个不同类型的 Volume。

### 使用 Volume 在 container 之间共享数据

我们先学习如何在一个 Pod 中的多个 containers 之间共享数据。

#### Using an emptyDir Volume

emptyDir 是最简单的 Volume 类型，它被初始化为一个空目录，然后运行在 Pod 中的 container 向其中写数据。emptyDir 在 containers 之间共享数据时非常有用。我们还是以上面介绍的例子，不过我们只关注 contentAgent 和 webServer 两个 containers。

我们使用 *Nginx* 作为 web server，用 UNIX *fortune* 命令来生成 HTML 页面，*fortune* 命令每次会随机打印出一串字符。我们写一个脚本每十秒调用一次 *fortunr* 命令。Nginx 镜像和 fortune 镜像都已经准备在 DockerHub 上了。下面我们就创建这个 Pod。

```yml
apiVersion: v1
kind: Pod
metadata:
  name: fortune
spec:
  containers:   # fortune镜像创建html-generator容器
  - image: luksa/fortune 
    name: html-generator 
    volumeMounts:   # html Volume挂载/var/htdocs目录
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine   # nginx镜像创建web-server容器
    name: web-server 
    volumeMounts:   # html Volume同样挂载在/usr/share/nginx/html目录
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true    # 设置为只读
    ports:
    - containerPort: 80 
      protocol: TCP
  volumes:      # 这里创建Volume
  - name: html
    emptyDir: {}
```

上面的 Pod 包含两个 containers 和一个 Volume。当 html-generator 容器开始工作时，它把 *fortune* 的输出写到 */var/htdocs/index.html*。因为该目录被挂载在 Volume，所以 index.html 文件被写到 Volume 上。当 web-server 容器开始工作时，它就以 /usr/share/nginx/html 目录（这是 Nginx 的默认目录）下的文件生成 web server，因为已经把 Volume 挂载到这个目录，Nginx 就以 html-generator 容器写的 index.html 文件生成 web server，并监听 Pod 的 80 端口。所以当 client 向 Pod 的 80 端口发送 HTTP 请求时，将收到 *fortune* 信息作为响应。

为了看到 *fortune* 响应，我们把 Pod 的端口映射到本机端口。

```
$ kubectl port-forward fortune 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
```

然后访问本机的 8080 端口。可以看到该 Pod 在正常工作。

```
$ curl http://localhost:8080
Beware of a tall blond man with one black shoe.
```

emptyDir 类型的 Volume 是被创建在运行该 Pod 的 Worker Node 的磁盘上，所以它的性能依赖于磁盘的性能。你也可以通过以下描述告诉 Kubernetes 创建一个内存上的 emptyDir。

```
volumes:      
- name: html
  emptyDir: {}
    medium: Memory
```

#### Using a Git Repository

gitRepo Volume 是一个用 clone 的 git 仓库填充的 emptyDir Volume，如下图所示。

![img](/img/post/post_gitrepo.png)

注意：在 gitRepo Volume 被创建之后，它就不会再和 Git 仓库保持同步了。当你往仓库中 push 提交之后，Volume 的内容不会更新。然而，如果 Pod 通过 RC 管理，删除 Pod 可以使 RC 新创建 Pod，同时 pull Git 仓库的内容创建 Volume。这时新创建的 Volume 会包含最新的内容。

可以使用一个 Git 仓库存储静态的 HTML 页面，然后创建一个 Pod 包含一个 Web Server container 和一个 gitRepo Volume。每次 Pod 被创建时，都会 pull 最新的 HTML 页面。接下来我们将实现这个示例。

已经有一个 HTML 文件的 Git 仓库 *https://github.com/luksa/kubia-website-example.git*，只需要 fork 到自己的账户下即可。然后就像之前一样创建一个 Pod，只不过现在这个 Pod 包含一个 Web Server 容器和一个 gitRepo Volume。gitRepo 指向自己 fork 的 git 仓库。

```yml
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod 
spec:
  containers:
    - image: nginx:alpine
      name: web-server 
      volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
          readOnly: true
      ports:
      - containerPort: 80 
        protocol: TCP
  volumes:
    - name: html
      gitRepo:  # 在创建一个gitRepo Volume
        repository: https://github.com/haozhangms/  kubia-website-example   # Volume将clone这个仓库 
        revision: master    # 检索master分支
        directory: .    # 仓库被clone到Volume的根目录
```

当创建 Pod 时，Volume 先被初始化为空目录，然后指向的 Git 仓库 clone 进去。另外，我们也要指定目录为 .（当前目录），否则仓库会被 clone 到 Volume 的子目录中；还要记得指定仓库的哪一个分支。

当 Pod 正常运行后，就可以通过 Port forwarding 验证 Pod 了。

和 emptyDir 一样，gtiRepo Volume是一个 Pod 特定的目录，当 Pod 消亡时 Volume 也会消亡。然而其他类型的 Volume 不会创建一个新目录，而是挂载一个已有的外部目录到 Pod 的容器的文件系统上。这种 Volume 的内容就不会随 Pod 消亡而消亡，可以存活多个 Pod 的生命周期。接下来我们将学习。

### 访问 Worker Node 文件系统上的文件

大多数 Pod 都不会和 host Node 交互，所以不应该访问 Node 上的文件系统。但是某些系统级的 Pod（通常由 DaemonSet 管理）确实需要读取 Node 上的文件或使用 Node 的文件系统。Kubernetes 因此提出了 hostPath 类型的 Volume。

#### Introducing the hostPath volume

一个 hostPath Volume 指向 host Node 文件系统的特定目录。运行在相同 Node 并且使用相同路径的 hostPath Volume 将看到相同的目录。如下图所示。

![img](/img/post/post_hostpath.png)

因为 emptyDir 和 gitRepo 的生命周期和 Pod 一样，而 hostPath Volume 的内容不会随 Pod 的消亡而消亡，所以 hostPath 是我们介绍的第一种持久存储。如果一个创建了 hostPath Volume 的 Pod 被删除了，下一个 Pod 的 hostPath Volume 指向了相同的路径，并且它被调度到了相同的 Node 上，那么它可以看到上一个 Pod 留下的数据。

但如果你想用 hostPath Volume 来保存数据库的数据，还请慎重。因为 hostPath Volume 的内容被保存在特定的 Node 上，如果 Pod 被迁移到其他 Node 上，它将看不到这些数据。

#### Examing system Pods that use hostPath Volume

我们看看 hostPath Volume 是如何被使用的。我们不用新创建 Pod，只需查看 *kube-system* 命名空间的 Pod。

```
$ kubectl get pod  --namespace kube-system 
NAME                               READY   STATUS    RESTARTS   AGE
coredns-f9fd979d6-g824g            1/1     Running   1          3d1h
etcd-minikube                      1/1     Running   1          3d1h
kube-apiserver-minikube            1/1     Running   1          3d1h
kube-controller-manager-minikube   1/1     Running   1          3d1h
kube-proxy-cspt2                   1/1     Running   1          3d1h
kube-scheduler-minikube            1/1     Running   1          3d1h
storage-provisioner                1/1     Running   5          3d1h
```

我们选择 *etcd-minikube* Pod 查看它的描述文件。可以看到，这个 Pod 使用了两个 hostPath Volume，分别访问两个目录。

```
....
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/minikube/certs/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/minikube/etcd
    HostPathType:  DirectoryOrCreate
....
```

hostPath Volume 主要用来访问 Node 的数据。当然它也可以尝试用来在单节点集群上持久存储，比如 Minikube。之后我们将介绍在集群中（不管单节点还是多节点）持久存储的方法。

### Using persistent storage

我们之前介绍的几种 Volumes 都不能持久化数据让其他 Node 上的 Pod 也能访问。要想数据能被所有的集群 Node 访问，那它必须存储在 network-attached storage (NAS)。本节我们介绍几种 NAS。

为了学习这类 Volume，我们创建一个运行 MongoDB 面向文档数据库的 Pod，然后为了存储持久化数据，我们需要给 MongoDB 容器挂载一个合适类型的 Volume。

#### Using a GCE Persistent Disk in a pod volume

我们先创建一个 GCE 持久化磁盘，它需要和你的集群区域相同。你可以查看通过 *gcloud* 命令查看集群的区域。

```
$ gcloud container clusters list
NAME    ZONE             MASTER_VERSION   MASTER_IP ...
kubia   europe-west1-b   1.2.5            104.155.84.137 ...
```

上面显示创建的区域在 *europe-west1-b*，所以我们创建相同区域的磁盘。

```
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b mongodb
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. 
For more information, see: https://developers.google.com/compute/docs/disks#pdperformance.
Created [https://www.googleapis.com/compute/v1/projects/rapid-pivot- 136513/zones/europe-west1-b/disks/mongodb].
NAME     ZONE            SIZE_GB   TYPE         STATUS
mongodb  europe-west1-b  1         pd-standard  READY
```

我们创建了一个 1GB 大小的名为 mongodb 的磁盘。接下来我们创建 Pod 并将磁盘挂载在 Pod 上面。

```yml
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  volumes:
  - name: mongodb-data  # 创建的Volume名字
      gcePersistentDisk:
        pdName: mongodb  # 名字要对应之前创建的GCE磁盘名字
        fsType: ext4   # 一种Linux文件系统类型
  containers:
  - image: mongo
    name: mongodb 
    volumeMounts:
    - name: mongodb-data  # 挂载上面的Volume
      mountPath: /data/db   # MongoDB存储数据的路径
      ports:
      - containerPort: 27017
        protocol: TCP
```

该 Pod 包含一个容器和一个Volume，Volume对应的外部磁盘是创建的 GCE 持久化磁盘，如下图所示。

![img](/img/post/post_gce.png)

现在已经创建好了 Pod 和 GCE 持久化磁盘，我们可以在容器中执行 MongoDB 命令写数据。

```bash
$ kubectl exec -it mongodb mongo  # 容器中执行shell命令
MongoDB shell version: 3.2.8
connecting to: mongodb://127.0.0.1:27017 Welcome to the MongoDB shell.
For interactive help, type "help".
For more comprehensive documentation, see
http://docs.mongodb.org/ Questions? Try the support group
http://groups.google.com/group/mongodb-user
...
> use mystore   
switched to db mystore
> db.foo.insert({name:'foo'})   # 插入一条数据
WriteResult({ "nInserted" : 1 })
> db.foo.find()   # 查看插入的数据
{ "_id" : ObjectId("57a61eb9de0cfd512374cc75"), "name" : "foo" }
```

现在我们已经在数据库中插入了一条数据。我们可以删除这个 Pod，重新创建来查看数据库是否能看到该条数据。

```bash
$ kubectl delete pod mongodb    # 删除 Pod
pod "mongodb" deleted
$ kubectl create -f mongodb-pod-gcepd.yaml
pod "mongodb" created   # 重新创建 Pod
$ kubectl exec -it mongodb mongo
MongoDB shell version: 3.2.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
...
> use mystore
switched to db mystore
> db.foo.find()   # 可以看到数据仍然存在
{ "_id" : ObjectId("57a61eb9de0cfd512374cc75"), "name" : "foo" }
```

#### Using other types of volumes with underlying persistent storage

如果 Kubernetes 集群建在 AWS 上，可以使用  awsElasticBlockStore Volume 提供 Pod 的持久化存储；如果是 Azure 上，可以使用 azureFile 或 azureDisk Volume。流程和上面的基本相似。我们只需要把 Pod 描述中 Volume 的描述修改为对应的类型。

```yml
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  volumes:
  - name: mongodb-data
    awsElasticBlockStore:  # 假设使用 aws
      volumeId: my-volume  # aws 磁盘的名字
      fsType: ext4
  containers:
....
```

如果使用自己的服务器，也也有很多选择来挂载磁盘。例如，为了挂载一个 NFS share，只需要指定 NFS server 和 server 暴露的路径。

```yml
volumes:
  - name: mongodb-data  # Volume名字
    nfs:
      server: 1.2.3.4   # NFS server的IP
      path: /some/path  # NFS server暴露的路径
```

### 解耦 Pod 和底层的存储技术

上一节提到的持久化 Volume 要求开发者要知道底层存储的基础设施。如果要创建 NFS Volume，那你要知道它的服务器。但这违反了 Kubernetes 向上层开发者隐藏底层资源的初衷。理想情况下，开发者不需要知道底层的存储技术，也不需要知道运行 Pod 的真实服务器。当他需要持久化存储时，应该就像创建 Pod 时要 CPU、内存等资源那样，直接向 Kubernetes 要就可以了。

#### PersistentVolumes and PersistentVolumeClaims

为了让开发者直接要存储，而不关注具体的底层基础设施，Kubernetes 引入另外两个资源：PersistentVolumes and PersistentVolumeClaims。它们具体的联系如下图所示。

![img](/img/post/post_PersistentVolumes.png)

首先，建立底层存储的集群管理员添加特定的 Volume，然后通过创建 PersistentVolume 资源把它注册到集群中。当创建 PersistentVolume 时，管理员指定它的 size 和访问模式。

当集群使用者需要持久化存储时，首先创建一个 PersistentVolumeClaim，指定它的最小 size 和访问模式。然后把该 Claim 提交到 Kubernetes API Server。Kubernetes 找到合适的 Volume，并把它们绑定。

之后 PersistentVolumeClaim 就被用过 Pod 的持久化存储，在被解绑之前，其他用户不可以使用相同的 PersistentVolume。

#### Creating a PersistentVolume

回顾上一节的 MongoDB 的例子，这里我们不要在 Pod 中直接挂载 GCE 持久化磁盘了。首先，由集群管理员创建 GCE 磁盘并创建 PersistentVolume。然后使用者创建 PersistentVolumeClai 在 Pod 中使用。

上一节已经创建了 GCE 磁盘，这里我们创建对应的 PersistentVolume。

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv 
spec:
  capacity:
    storage: 1Gi  # PersistentVolume的size
  accessModes:    
  - ReadWriteOnce   # 要么是单个client读写
  - ReadOnlyMany    # 要么是多个client只读
  persistentVolumeReclaimPolicy: Retain  # claim释放后，volume被保留。
  gcePersistentDisk:
    pdName: mongodb   # 对应GCE磁盘的名字
    fsType: ext4
```

在用 *kubectl create* 创建之后，这个 PersistentVolume 已经准备好被 claim 了。

```
$ kubectl get pv
NAME        CAPACITY  RECLAIMPOLICY  ACCESSMODES  STATUS      CLAIM
mongodb-pv  1Gi       Retain         RWO,ROX      Available
```

PersistentVolume 不属于任何 Namespace，它类似于 Node，是 cluster-level 的资源。

#### Claiming a PersistentVolume by creating a PersistentVolumeClaim

现在我们要使用存储，就要通过 claim PersistentVolume。claiming PersistentVolume 是和 create Pod 独立的操作，因为我们要确保即便 Pod 被 reschedule (means previous pod is deleted and a new one is created)，我们的 PersistentVolume 也要访问到。

通过 *kubectl create* 将下面的描述文件提交到 API Server。

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
spec:
  resources:
    requests:
      storage: 1Gi  # 需要1G存储
  accessModes:
  - ReadWriteOnce   # 单client读写
  storageClassName: ""    # 之后解释
```

一旦创建了 claim，集群就会找合适size和访问模式的 PersistentVolume 与之绑定。查看 PersistentVolumeClaim 可看到已经绑定了 mongodb-pv。

```
$ kubectl get pvc
NAME          STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE
mongodb-pvc   Bound     mongodb-pv    1Gi       RWO,ROX       3s
```

也可以查看 PersistentVolume，如下可看到已经绑定了 default/mongodb-pvc，default 代表这个 PersistentVolumeClaim 所在的 Namespace。PersistentVolume 是 cluster-level 资源，但 PersistentVolumeClaim 是特定 Namespace 的资源，它只可以被相同 Namespace 的 Pod 使用。

```
$ kubectl get pv
NAME       CAPACITY   ACCESSMODES  STATUS  CLAIM                 AGE
mongodb-pv 1Gi        RWO,ROX      Bound   default/mongodb-pvc   1m
```

上面的访问模式用了缩写：
+ RWO：ReadWriteOnce，只有一个 Node 可以挂载 Volume 读写；
+ ROX：ReadOnlyMany，多个 Node 可以挂载 Volume 只读；
+ RWX：ReadWriteMany，多个 Node 可以挂载 Volume 读写；

注意上面的访问模式针对于同时访问的 Node 数目，而非 Pod 数目。

#### Using a PersistentVolumeClaim in a pod

现在 PersistentVolume 已经准备好使用了。在释放之前别的 Node 不可能绑定它。我们需要在 Pod 的描述中引用这个 PersistentVolumeClaim。

```yml
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  containers:
  - image: mongo
    name: mongodb 
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017 
    protocol: TCP
  volumes:
  - name: mongodb-data
    persistentVolumeClaim:    # 在Pod描述文件的volume位置引用PersistentVolumeClaim的名字
      claimName: mongodb-pvc
```

现在创建 Pod，然后看看是否连接的是之前使用的 GCE 持久化磁盘。如下所示。

```bash
$ kubectl exec -it mongodb mongo
MongoDB shell version: 3.2.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
...
> use mystore
switched to db mystore
> db.foo.find()   # 可以查看到之前插入的数据记录
{ "_id" : ObjectId("57a61eb9de0cfd512374cc75"), "name" : "foo" }
```

#### Understanding the benefits of using PersistentVolumes and claims

总结一下，下图展示了一个 Pod 直接和间接使用 GCE 持久化存储的两种方式。可以看到，虽然通过 PersistentVolumes 和 claims 间接引用多了额外的步骤，但是开发者不用关注底层的存储设施。而且，由于不用关注底层的存储设施实现方式，Pod 和 claim 的描述文件可以被重复使用。claims 中只需要指定：我需要多少 size 的存储，一次允许单个 Node 读写；然后 Pod 中引用这个 claim 即可。

![img](/img/post/post_pvandpvc.png)

#### 回收利用 PersistentVolumes

本节最后，我们删除这个 Pod 和 PersistentVolumeClaim。

```
$ kubectl delete pod mongodb
pod "mongodb" deleted
$ kubectl delete pvc mongodb-pvc
persistentvolumeclaim "mongodb-pvc" deleted
```

如果我们再次创建 PersistentVolumeClaim 会发生什么？它是否还会再绑定到 PersistentVolume。我们创建 claim 之后查看一下。

```
 $ kubectl get pvc
NAME          STATUS    VOLUME    CAPACITY   ACCESSMODES   AGE
mongodb-pvc   Pending                                     13s
```

可以看到，状态列显示 Pending。没有绑定到 PersistentVolume。这是为什么呢？当我们刚开始创建 claim 时就立刻绑定了。现在重新创建了就不能绑定了。我们再查看一下 PersistentVolume。

```
$ kubectl get pv
NAME        CAPACITY  ACCESSMODES  STATUS     CLAIM                 REASON  AGE
mongodb-pv  1Gi       RWO,ROX      Released   default/mongodb-pvc           5m
```

可以看到，PersistentVolume 状态是 Released，不是之前的 Available。因为你已经用过这个 Volume，它里面可能包含数据，所以不应该直接绑定到一个全新的 claim 上。否则，这个新创建的 claim 和引用该 claim 的 Pod 可能会访问到这些数据。

这是由我们创建 PersistentVolume 时的 persistentVolumeReclaimPolicy 属性决定的，我们设置了 *Retain*，告诉 Kubernetes 当该 Volume 释放后应该保留 volume 和其中的内容。所以手动回收这个 PersistentVolume 的唯一方法是删除并重新创建 PersistentVolume 资源。

还有两种 reclaim 设置：*Recycle* 和 *Delete*。第一种会删除 Volume 的内容使它可以再去绑定，工作流程如下图所示。第二种会在删除 claim 时也删除绑定的 Volume。

![img](/img/post/post_reclaim_retain.png)

### 动态配置 PersistentVolumes

上一节介绍了 PersistentVolumes 和 claims 的使用来帮助开发者无需关注底层的存储基础设施。但这仍需要集群的管理员事先创建 PersistentVolumes，其实 Kubernetes 可以通过动态配置 PersistentVolumes 自动完成这项工作。

集群管理只需部署一个 PersistentVolume provisioner，然后定义一些 StorageClass 对象让用户选择哪种类型的 PersistentVolume。用户在 claim 中引用 StorageClass，然后 provisioner 会创建PersistentVolume。

StorageClass 和 PersistentVolume 一样，也是 cluster-level 资源。目前各大云提供商的 Kubernetes 都提供了 provisioner，如果你是本地搭建的 Kubernetes，那你要自己部署一个。

#### Defining the available storage types through StorageClass resources

在 claims 创建之前，集群管理员要先创建几个 StorageClass。

```yml
apiVersion: storage.k8s.io/v1 
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd  # 用于配置Volume的插件
parameters:  # 传递给provisioner的参数
  type: pd-ssd
  zone: europe-west1-b
```

StorageClass 资源指定了 provisioner 和传递给 provisioner 的参数。这里使用 Google 的，使用其他云提供商的 Kubernetes 要用对应的 provisioner。

#### Requesting the StorageClass in a PersistentVolumeClaim

创建了 StorageClass 之后，可以在 claim 描述文件中引用它。

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
spec:
  storageClassName: fast  # 这里引用 StorageClass
  resources:
    requests:
      storage: 100Mi
  accessModes:
  - ReadWriteOnce
```

除了指定资源的 size 和访问模式之外，也指定了要使用的 StorageClass。当创建该 claim 时，*fast* StorageClass 中的 provisioner 就创建合适的 PersistentVolume。即便已经有手动配置好的合适的 PersistentVolume，provisioner 也还是会被调用。

可以查看 volume 和 claim。

```
$ kubectl get pvc mongodb-pvc
NAME          STATUS  VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS
mongodb-pvc   Bound   pvc-1e6bc048   1Gi        RWO           fast
$ kubectl get pv
NAME           CAPACITY ACCESSMODES RECLAIMPOLICY STATUS   STORAGECLASS 
pvc-1e6bc048   1Gi      RWO         Delete        Bound    fast
```

除了 PV 以外，还可以查看 provisioner，它配置了 GCE 持久化存储。

```
$ gcloud compute disks list
NAME                        ZONE            SIZE_GB   TYPE      STATUS
gke-kubia-dyn-pvc-1e6bc048  europe-west1-d  1         pd-ssd    READY
```

#### Dynamic provisioning without specifying a storage class

其实，Kubernetes 中已经配置了一个默认的 StorageClass。当我们没有指定使用哪个 StorageClass 时就会使用默认的。

```
$ kubectl get sc
NAME                TYPE
fast                kubernetes.io/gce-pd
standard (default)  kubernetes.io/gce-pd
```

下面我们创建一个不指定 StorageClass 的 PVC。

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc2 
spec:   # 这里没有设置StorageClass属性
  resources:
    requests:
      storage: 100Mi
  accessModes:
  - ReadWriteOnce
```

当上面的 PVC 创建时，就会调用默认的 StorageClass 创建 PV。可以查看验证。

```
$ kubectl get pvc mongodb-pvc2
NAME            STATUS    VOLUME         CAPACITY   ACCESSMODES  STORAGECLASS 
mongodb-pvc2    Bound     pvc-95a5ec12   1Gi        RWO          standard
$ kubectl get pv pvc-95a5ec12
NAME          CAPACITY   ACCESSMODES    RECLAIMPOLICY  STATUS    STORAGECLASS 
pvc-95a5ec12  1Gi        RWO            Delete         Bound     standard
```

如果我们强制 PVC 绑定一个已经手动配置好的 PersistentVolume。我们就要像在上面最开始创建 PVC 的时候指定 storageClassName 属性为空字符串。

```yml
kind: PersistentVolumeClaim
spec:
  storageClassName: ""  # 确保PVC绑定一个已经配置好的PV，而非是动态provisioner创建的
```

在最后三节，我们一直在讲如何在 Pod 中使用持久化存储。最后这里我们总结一下，Pod 使用持久化存储的最佳方式应该是，只创建 PVC 和 Pod。PVC 如果有必要就用特定的 SrorageClass，否则就用默认的。Pod 中按名字引用 PVC。底层存储的所有都交给动态 provisioner 配置。完整的流程图如下图所示。

![img](/img/post/post_dynamic_provisioner.png)

参考自：
1. 《Kuberneter in Action》 by Marko Luksa.

