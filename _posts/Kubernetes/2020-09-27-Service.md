---
layout:     post
title:      Chapter 9. Service
subtitle:   enabling clients to discover and talk to pods
date:       2020-09-27
author:     Hao
header-img: img/post/post_bg_coffee.jpg
catalog: true
mathjax: true
tags:
    - Kubernetes
---

截止到目前，我们已经学习了 Pod 和部署管理 Pod 的各种 Controllers。即便某些 Pod 是在集群内部独立运行的，但现在的大多数 Pod 都是要与集群外部交互的，例如 Pod 要响应集群外部 client 的 HTTP 请求。在 Kubernetes 出现之前，每一个微服务应用是由 Ops 团队配置一个 IP 和 Port，使得它可以被其他服务发现和访问。但这样的配置方式不适用于 Kubernetes，原因有以下几点：

+ Pod 生命周期短。Pod 随时可能因为各种原因(Node 失效，Node 空间不足)被创建、移动和终止。
+ Pod 被调度到某个 Node 之后启动之前，Kubernetes 才会分配给它 IP 地址。所以外部的 client 事先不知道 Pod 的 IP。
+ 如果 Pod 被 scale out，多个 Pod 会提供相同的应用服务。但每个 Pod 都会有自己的一个 IP，外部 client 不应该关注有多少个 Pod 可以访问，或应该访问哪一个，而是应该以同一个 IP 访问同一个应用服务。

为了解决这个问题，Kubernetes 提出了另一个资源，Service。

### 引入 Service

Service 是访问一组提供相同服务的 Pod 的入口，当 Service 被创建后，它会被分配一个在整个生命周期都不会变的 IP 和 Port，client 可以 连接 Service 来访问 Service 背后的服务。这样，Service 就把 client 和 Pod 解耦。client 不用关注 Service 背后有多少个 Pod，或者它们在集群中的哪个位置，Pod 也可以随时移动或被销毁。

下图展示了一个前端 web server 和后端 database 的例子。可能会有多个 Pod 充当前端，但只有一个 database 做后端数据库。而我们要做到的是，client 不用关注自己连接的是哪一个前端 web server，同样地，保存 database 的 Pod 可能会被移动，而这些动作对前端来说是透明的。

![img](/img/post/post_service_intro.png)

所以，我们为前端的 Pods 创建一个 Service，使得 client 可以通过单一不变的 IP 地址访问 web server。类似地，为后端创建 Service，前端可以恒定的访问后端 Servie，而不用关注 database 的 Pod 可能被移动。

### 创建 Service

为了让 Service 选择多个运行相同应用的 Pod，我们同样像 ReplicationController一样使用 label selector。如下图，Service 选择管理三个具有 app=kubia 标签的 Pods。

![img](/img/post/post_service_labelselector.png)

创建一个 Service 的最快方法是我们之前用过的 *kubectl expose* 命令，*expose* 命令会创建一个与指定的 Pod 或 ReplicationController 有相同 pod selector 的 Service。这里，我们通过以下的描述文件手动创建一个 Service。**spec.selector** 选择要管理哪些 Pod，**Port** 指定了 Service 向外暴露的端口，**targetPort** 指定了 Service 监听对应容器的端口。

```yml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:   
  - port: 80    # The port this service will be available on
    targetPort: 8080    # The container port the service will forward to
  selector:
    app: kubia
```

通过 *kubectl create* 命令创建之后，我们就创建了一个 Service，该 Service 监听 80 端口的连接，并把每条连接都 route 到匹配 *app: kubia* label 的一个 Pod 8080 端口。

我们可以通过 *kubectl get service* 来看看自己创建的 Service。

```
kubectl get svc
NAME        CLUSTER-IP      EXTERNAL-IP     PORT(S)     AGE 
kubernetes  10.111.240.1    <none>          443/TCP     30d 
kubia       10.100.253.77  <none>          80/TCP      6m
```

#### 集群内部测试 Service

可以看到列表中给我们创建的 Service 指定了一个集群 IP 地址，这个集群地址只能在集群内部访问。所以，我们现在先从内部测试一下这个 Service。从集群内部向 Service 发请求一共有以下几种方式：

+ 创建一个向 Service 集群 IP 发请求的 Pod，并 log 响应消息。通过查看响应信息的 log 测试 Service 的响应。
+ 可以 ssh 到一个 Pod 内部，在这个 Pod 上 curl Service IP:Port。
+ 通过 *kubectl exec* 命令在一个 Pod 上执行 curl Service IP:Port。

这里我们选择最后一种方式来测试。*kubectl exec* 命令可以让我们在 Pod 内部远程执行命令。我们需要选择一个执行命令的 Pod，还要知道 Service 的集群 IP 地址，然后即可输入以下命令。

```
kubectl exec kubia-7nog1 -- curl -s http://10.100.253.77
You've hit kubia-gzwli
```

可以看到，在名为 kubia-7nog1 的 Pod 中，我们 curl 了目标 Service 的 IP 地址，Service 将这个请求 route 到背后代理的任意一个 Pod（这里route 到了 kubia-gzwli Pod）。之后这个 Pod 做出响应，打印出输出结果。整体的流程如下图所示。

![img](/img/post/post_service_kubectl_exec.png)

#### 配置 Service 的 Session Affinity

如果我们多次执行命令，应该能 hit 到不同的 Pod。因为 Service 代理每次都随机 route 到一个 Pod，即便连接来自同一个 client。某些情况下，如果我们需要让同一个 client 的请求都被 route 到同一个 Pod 上，我们可以设置 *sessionAffinity* 属性，该属性默认为 None。

```
apiVersion: v1
kind: Service
spec:
  sessionAffinity: ClientIP
```

#### 一个 Service 暴露多个端口

现在 Service 只暴露了一个端口，但 Service 也可以支持多个端口。例如，一个 Pod 监听 HTTP 8080 和 HTTPS 8443 两个端口，我们可以使用一个 Service 分别把 80 端口和 443 端口路由到 Pod 的两个端口，无需创建两个 Service。多端口 Service 的描述文件如下所示。

```
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec: 
  ports:
  - name: http 
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia
```

注意：
+ 当创建一个多端口的 Service 时，每个端口必须指定一个名字。
+ label selector 是应用于整个 Service，不能为 Port 单独配置。

#### Named Ports

在上面的几个例子中，我们可以看到，端口不仅可以通过端口号引用，也可以通过指定一个端口名字来引用。用一个端口名字可以使描述更清晰，并且修改更简单。例如，我们可以为 Pod 指定端口名字，在 Service 描述中引用端口名。当修改 Pod 监听的端口时，我们不用修改 Service 的描述。

如下展示了 Pod 描述指定端口名和 Service 中引用的示例。

```
kind: Pod
spec:
  containers:
  - name: kubia
    ports:
    - name: http
      containerPort: 8080
    - name: https
      containerPort: 8443

apiVersion: v1
kind: Service
spec:
  ports:
  - name: http
    port: 80
    targetPort: http
  - name: https
    port: 443
    targetPort: https
```

### 发现 Service

通过创建 Service，我们已经了解到可以通过一个固定的 IP 地址和 Port 去访问 Service 背后的 Pod。背后的 Pod 可以移动、销毁、数目可以变化等等。但是，client Pod 如何知道一个 Service 的 IP 地址和 Port 呢？在我们之前用 *kubectl exec* 测试的时候，我们是手动的填了 IP 地址。其实，Kubernetes 也提供了 client Pod 发现 Service IP: Port 的方法。

#### through Environment Variables

当一个 Pod 被创建时，Kubernetes 会给它初始化一个环境变量集合，其中就包括了当前时刻存在的 Service 的 IP 地址和 Port。

刚才我们是先创建 Pod，再创建了 Service。所以，Pod的环境变量没有设置 Service 的信息。我们可以删除所有的 Pod，让 ReplicationControlller 再创建出三个新的。 此时，我们查看任意一个 Pod 的环境变量。

```
$ kubectl delete po --all
pod "kubia-7nog1" deleted 
pod "kubia-bf50t" deleted 
pod "kubia-gzwli" deleted

$ kubectl exec kubia-3inly env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=kubia-3inly
KUBERNETES_SERVICE_HOST=10.111.240.1 KUBERNETES_SERVICE_PORT=443
....
KUBIA_SERVICE_HOST=10.100.253.77
KUBIA_SERVICE_PORT=80
....
```

可以从输出结果中看到，此时集群中定义了两个 Service：kubernetes 和 kubia，因此在环境变量集合中发现了这两个 Service 的 IP 地址和 Port。

#### through DNS

事实上，Kubernetes 也建立一个 DNS 服务器，让你可以查询 Service 的 IP 地址和 Port。我们可以查看 Kubernetes 中的 Namespace，发现有一个名为 kube-system 的命名空间，并且这个 Namespace 中有名为 kube-dns 的 Service。

```
$ kubectl get ns
NAME                   STATUS   AGE
default                Active   46h
kube-node-lease        Active   46h
kube-public            Active   46h
kube-system            Active   46h
kubernetes-dashboard   Active   45h

$ kubectl get svc -n kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   46h
```

因此，这个 Service 背后的 Pod 运行着一个 DNS 服务器，它知道集群中所有 Service 的信息。集群中任意一个 Pod 的 DNS 查询最终都路由到这个 DNS 服务器来处理。每个 Service 都在内部的 DNS 服务器上注册了一个 entry，client Pod 如果知道 Service name，就可以通过 FQDN (fully qualified domain name) 来访问这个 Service。

FQDN 格式为 service name.namespace.cluster domain suffix。第一个字段是 Service 的名字，第二个字段是所在的命名空间，第三个字段是一个集群域名前缀。

如果 Pod 和访问的 Service 在同一 Namespace，我们可以省略后两个字段，直接通过 Service 名字来访问，如下所示。

```
$ kubectl exec -it kubia-72nr8 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@kubia-72nr8:/# curl http://kubia.default.svc.cluster.local
You've hit kubia-cg2jq
root@kubia-72nr8:/# curl http://kubia.default
You've hit kubia-qtl7f
root@kubia-72nr8:/# curl http://kubia
You've hit kubia-cg2jq
```

这里我们先使用 *kubectl exec* 命令和 *-it* 参数在一个 Pod 的容器内运行 bash，然后再测试访问 Service。

最后在这再多说一句，我们在测试访问 Service 的时候都是用 curl。这里不能使用 ping 来测试连接性，因为 Service 的集群 IP 是一个虚拟 IP，只有和 Port 一起使用它才有意义。

### Service 连接集群外部的服务

存在这样一种情况：我们想要通过 Kubernetes 的 Service 暴露集群外部的服务。也就是说，Service 不是把监听的连接路由到集群内部的 Pod，而是重定向到外部的 IP 地址和 Port。

#### 引入 Endpoints

事实上，Service 并没有直接和 Pod 连接。如果我们使用 *kubectl describe svc* 命令查看某个 Service，就会发现在他们之间存在另一种资源：Endpoints，如下所示。

```yml
$ kubectl describe svc kubia
Name:              kubia
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          app=kubia    # service的pod selector用来创建endpoints
Type:              ClusterIP
IP:                10.100.253.77
Port:              <unset>  80/TCP
TargetPort:        8080/TCP
Endpoints:         172.17.0.10:8080,172.17.0.11:8080,172.17.0.9:8080    #IP地址和端口的列表
Session Affinity:  None
Events:            <none>
```

Endpoints 资源不是 Service 的属性，它也是一种 Kubernetes 资源。它是一个 IP 地址和端口组成的列表。可以用 *kubectl get* 命令查看。

```
$ kubectl get endpoints kubia
NAME    ENDPOINTS                                           AGE
kubia   172.17.0.10:8080,172.17.0.11:8080,172.17.0.9:8080   3h17m
```

即便 Service 的描述文件中指定了 Pod Selector，在路由监听的连接时也并没有真正用到。相反，这个 pod selector 被用于建立一个 IP 地址和 Port 组成的列表，存储在 Endpoints 资源中。当 Service 监听到一条连接后，Service 代理就选择一个 IP:Port 将请求路由到监听这个 IP:Port 的服务器上。

#### 手动配置 Service Endpoints

让 Service 的 Endpoints 从 Service 中解耦，可以让它们手动的配置和更新。

如果你创建了一个没有 Pod Selector 的 Service，Kubernetes 就不会创建对应的 Endpoints。需要你手动创建，并指定 Endpoints 的 IP:Port 列表。

首先，我们创建一个没有 pod selector 的 Service。

```yml
apiVersion: v1
kind: Service
metadata:
  name: external-service 
spec:
  ports:
  - port: 80
```

然后为这个 Service 手动创建 Endpoints。

```yml
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service    # 名字要和Service的名字对应
subsets:
  - addresses:
    - ip: 11.11.11.11   # Service重定向连接的IP
    - ip: 22.22.22.22 
    ports:
    - port: 80
```

在 Service 和 Endpoints 都创建好之后，Service 就可以像有 Pod Selector 的正常 Service 一样使用了。在这个 Service 之后创建的 Pod 都会从环境变量集合中知道 Service 的 IP 地址和 Port。访问这个 Service 的所有连接也都会被这个 Endpoints 负载均衡。下图展示了三个 Pod 连接有外部 Endpoints 的 Service。

![img](/img/post/post_service_endpoints.png)

如果你之后想这个 Service 迁移到集群内部的 Pod 上，即让它代理内部的 Pod，你可以为这个 Service 添加一个 Pod Selector，从而让它自动管理 Endpoints。反之亦然，如果你移除了 Service 的 Pod Selector，Kubernetes 就停止更新它的 Endpoints。总之来说，可以在一个 Service 的 IP 地址保持不变的同时，更改其背后的真实实现。

#### 为集群外部 Service 设置别名

上一节我们通过手动配置 Endpoints 实现了集群外部 Service。这一节，我们学习如何直接通过 FQDN 来访问集群外部 Service，不需要手动配置 Endpoints。

首先，我们创建一个无 Pod Selector 的 Service，并把 *type* 属性设置为 *ExternalName*。例如，我们假如有一个可以访问 api.somecompany.com 的API，我们可以创建如下的 Service。

```yml
apiVersion: v1
kind: Service
metadata:
  name: external-service 
spec:
  type: ExternalName
  externalName: someapi.somecompany.com 
  ports:
  - port: 80
```

在这个 Service 创建之后，Pod 可以直接通过 *external-service.default.svc.cluster.local* 域名甚至 *external-service* 连接到外部 Service。这隐藏了真实的服务的实现，我们可以在 Service 的运行过程中只修改 *externalName* 属性、或将 *type* 改回 ClusterIP 来随时修改服务。这样的服务完全是在 DNS 层实现的。

### Service 暴露给外部的 client

以上我们介绍的是 Service 如何被集群内部的 Pod 消费。但有时我们也需要把 Service暴露给外面 client，例如，前端 server 需要暴露给外部。接下来我们介绍如何暴露 Service 使得集群外部的 client 可以访问。

![img](/img/post/post_service_expose_outside.png)

有以下几种方式可以从外部访问集群内部的 Service：
+ Service 类型设置为 *NodePort*。对于一个 *NodePort* Service，Service 背后代理的每个节点都会开一个端口，把该端口接收到的请求转发给该 Service。
+ Service 类型设置为 *LoadBalancer*，这是 *NodePort* 的扩展版。外部连接可以访问 LoadBalancer 的 IP，然后 LoadBalancer 将连接转发到某个 NodePort。
+ 通过 Ingress 资源。这个下一节再说。

#### NodePort Service

向外部 client 暴露 Service 的第一种方法是使用 NodePort 类型的 Service。顾名思义，NodePort 类型的 Service 使得 Kubernetes 在 Service 管理的所有 Node 上预留一个端口，并将访问这个端口的外部请求转发给 Service，Service 再将请求分配给某个 Pod。

我们可以通过如下描述文件创建 *NodePort* Serivce。Service 的类型指定为 NodePort，nodePort 的端口指定为 30123。

```yml
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport 
spec:
  type: NodePort ports:
  - port: 80    # Service监听的端口
    targetPort: 8080    # Service转发到Pod的端口
    nodePort: 30123   # NodePort新开的端口
  selector:
    app: kubia
```

我们可以观察创建的 Service。在 EXTERNAL-IP 一栏中显示 \<nodes\>，表示可以通过任意一个 Node 的 IP 来访问。PORT 栏显示了 cluster IP 的端口 80 和 Node Port 30123。所以 Service 可以通过以下几个地址访问：10.11.254.223:80 或者 <node’s IP>:30123。

```
$ kubectl get svc kubia-nodeport
NAME            CLUSTER-IP      EXTERNAL-IP     PORT(S)         AGE
kubia-nodeport  10.111.254.223  <nodes>         80:30123/TCP    2m
```

下图展示了一个外部请求连接 *NodePort* Service，要么通过 Node1，要么通过 Node2。

![img](/img/post/post_service_nodeport.png)

当使用 Minikube 时，可以使用如下命令访问 NodePort 类型的 Service。
```
minikube service <service-name> [-n <namespace>]
```

#### Load Balancer Service

*NodePort* Service 可以通过任意一个 Node 的 IP 地址和 Port 来访问，如上图所示。但如果我们的客户端一直是通过 Node1 来访问的，当 Node1 crash 了，此时 client 就不能访问 Service 了。所以，就需要在 Node 前面再加一层 Load Balancer 来确保请求被转发到正常运行的 Node 端口上。

一般云服务商提供的 Kubernetes 集群会自动提供 Load Balancer，Load Balancer 有自己独立的、可公开访问的 IP 地址，它会重定向所有到达 Service 的连接。如果 Kubernetes 运行的环境不支持 Load Balancer，它会自动转换为 NodePort 类型的 Service。这是因为 Load Balancer 其实是 Node Port 的扩展。要注意的是，Minkube 不支持 Load Balancer。

我们可以通过如下方式来创建一个 Load Balancer 类型的 Service。除了指定类型为 Load Balancer 以外没啥特别的。

```yml
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer 
spec:
  type: LoadBalancer 
  ports:
  - port: 80
    targetPort: 8080    # 即便我们可以指定NodePort，但这里我们也可以省略，让 Kubernetes 给我们随机选择一个
  selector:
    app: kubia
```

当利用 **kubectl create**命令创建后，云服务商会创建一个 Load Balancer，并把它的 IP 地址写入我们创建的 Service。我们查看创建的 Service，可以看到 EXTERNAL-IP 栏出现一个 IP 地址，我们就可以通过它访问 Service。如果显示 Pending 状态，表示正在分配 IP 地址，等一会儿再刷新看看。

```
$ kubectl get svc kubia-loadbalancer
NAME                CLUSTER-IP      EXTERNAL-IP       PORT(S)       AGE
kubia-loadbalancer  10.111.241.153  130.211.53.173    80:32143/TCP  1m
```

多说一句，因为 *LoadBalancer* Service 现在向外暴露了 Load Balancer 的地址，所以我们可以通过 web 游览器直接访问。有时我们会发现，游览器总是会 hit 相同的 Pod，即便我们的 Service 的 session affinity 属性是 None。这是因为，游览器使用 keep-alive 连接，所有的请求都是在一个连接中完成的。而每次 curl 时都会新开一个连接。

下图展示了对于一个 *LoadBalancer* Service，HTTP 请求如何被转发到 Pod 的过程。访问 Load Balancer 的 HTTP 请求首先被转发到 Node 分配的 Port 上，Node Port将请求转发给 Service，然后由 Service 路由到某一个 Pod 实例。

![img](/img/post/post_service_loadbalancer.png)

再补充一点，当一个外部 client 访问 *NodePort* 或 *LoadBalancer* Service时，最后选择的 Pod 有可能就是最初接收到连接的相同的 Pod。所以这中间会增加不必要的网络跳数。为了避免额外的跳数，我们可以设置如下属性。

```yml
spec:
  externalTrafficPolicy: Local
```

如果一个 Service 包含了这样的设置，一个外部 client 通过 NodePort 连接，那么 Service 代理就会选择这个 Node 上运行的 Pod。但如果本 Node 没有 Pod，那么这条连接就会被一只挂起，不会再被全局随机选择 Pod。因此，我们要确保 LoadBalancer 把请求转发到至少有一个运行 Pod 的 Node 上。

另外，Service 这样设置可能会造成负载不均衡。正常情况下，外部请求在全部的 Pod 上随机选择一个。但现在，假设有两个 Node，Node A 上有一个 Pod，Node B 上有两个 Pod，那么 Node A 可能接收 50% 的连接，Node B 上的两个 Pod 分别 25%。如下图所示。

![img](/img/post/post_service_local.png)

通常，集群内部的 client 连接一个 Service，Service 背后的 Pod 可以获得 client 的 IP 地址。但当通过 NodePort 连接时，包的源 IP 地址由于 SNAT(Source Network Address translation) 会被改变。对于某些场景这可能是个问题，例如在 web server 场景下，可能拿不到游览器的 IP 地址。

但上面的 Local 设置可以避免该问题。因为它只在本 Node 上运行，不需要经过 SNAT。

### Ingress 资源

除了以上两种方法向外暴露 Service 给外部的 client，我们还可以通过创建一个 Ingress 资源向外暴露 Service。每一个 Load Balancer Service 都需要自己的一个 Load Balancer，而一个 Ingress 可以提供多个 Service 的访问，只需要一个 Load Balancer。当一个请求发送到 Ingress，请求的 Host 和 Path 决定了它应该被转发到哪一个 Service，如下图所示。

![img](/img/post/post_service_ingress_exm.png)

Ingress 资源定义了上图中的各种路由规则，具体的规则定义请参考这一篇[文章](https://haozhangms.github.io/2020/08/14/Kubernetes-Ingress/)。但要想让 Ingress 中的路由规则工作，我们需要运行一个 Ingress Controller，不同的 Kubernetes 环境提供的 Ingress Controller 各不相同，一般我们用官方维护的 [Nginx](https://kubernetes.github.io/ingress-nginx/)。

下图展示了 Ingress Controller 的工作流程。

![img](/img/post/post_ingress_controller.png)

### Readiness 探针

一旦有合适 Label 的新 Pod 被创建，它就会成为某个 Service 的一部分，被重定向一些请求。但如果新创建的 Pod 由于创建时间比较长还没准备好开始想听请求呢？这该怎么办。

之前在介绍 ReplicationController 时我们学习了Liveness 探针，它被用来检测一个 Pod 是否健康，不健康的 Pod 会自动重启。这里我们引入另一个探针 Readiness，它会周期性的调用来检测 Pod 是否可以正常响应请求，当 Readiness 探针返回成功时，代表该 Pod 准备好接受外部请求了。

不同于 Liveness 探针，Readiness 探针失败时不会重启 Pod，它只会确保 Service 包含能正常响应请求的 Pod。如下图所示，当一个 Pod 的 Readiness 探针失败了，它就会被移出 Endpoint 对象，到达这个 Service 的客户端连接将不再会路由到这个 Pod。

![img](/img/post/post_deployment_1.png)

#### Create Readiness Probe

Readiness 与 Liveness 探针相似，都是在 spec.template.spec.containers 下创建。如下例所示，Readiness 探针会周期性的在容器中执行 ls /var/ready 命令，如果文件存在，命令返回码为0，表示成功，否则返回失败。

```yml
apiVersion: v1
kind: ReplicationController 
...
spec:
...
  template: 
  ...
  spec: 
    containers:
    - name: kubia
      image: luksa/kubia
      readinessProbe:
        exec:
          command:
          - ls
          - /var/ready
      ...
```

### 

参考自：
1. Kuberneter in Action by Marko Luksa.

