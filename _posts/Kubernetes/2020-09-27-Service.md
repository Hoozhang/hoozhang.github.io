---
layout:     post
title:      Chapter 9. Service
subtitle:   enabling clients to discover and talk to pods
date:       2020-09-27
author:     Hao
header-img: img/post/post_bg_coffee.jpg
catalog: true
mathjax: true
tags:
    - Kubernetes
---

截止到目前，我们已经学习了 Pod 和部署管理 Pod 的各种 Controllers。即便某些 Pod 是在集群内部独立运行的，但现在的大多数 Pod 都是要与集群外部交互的，例如 Pod 要响应集群外部 client 的 HTTP 请求。在 Kubernetes 出现之前，每一个微服务应用是由 deploy 团队配置一个 IP 和 Port，使得它可以被其他服务发现和访问。但这样的配置方式不适用于 Kubernetes，原因有以下几点：

+ Pod 生命周期短。Pod 随时可能因为各种原因(Node 失效，Node 空间不足)被移动和终止。
+ Pod 被调度到某个 Node 之后，在它启动之前 Kubernetes 才会分配给它 IP 地址。所以外部的 client 事先不知道 Pod 的 IP。
+ 如果 Pod 被 scale out，多个 Pod 会提供相同的应用服务。但每个 Pod 都会有自己的一个 IP，外部 client 不应该关注有多少个 Pod 可以访问，或应该访问哪一个，而是应该以同一个 IP 访问同一个应用服务。

为了解决这个问题，Kubernetes 提出了另一个资源，Service。

### 引入 Service

Service 是访问一组提供相同服务的 Pod 的入口，当 Service 被创建后，它会被分配一个在整个生命周期都不会变的 IP 和 Port，client 可以 连接 Service 来访问 Service 背后的服务。这样，Service 就把 client 和 Pod 解耦。client 不用关注 Service 背后有多少个 Pod，或者它们在集群中的哪个位置。Pod 也可以随时移动或被销毁。

下图展示了一个前端 web server 和后端 database 的例子。可能会有多个 Pod 充当前端，但只有一个 database 做后端数据库。而我们要做到的是，client 不用关注自己连接的是哪一个前端 web server，同样地，保存 database 的 Pod 可能会被移动，而这些动作对前端来说是透明的。

![img](/img/post/post_deployment_1.png)

所以，我们为前端的 Pods 创建一个 Service，使得 client 可以通过单一不变的 IP 地址访问 web server。类似地，为后端创建 Service，前端可以恒定的访问后端 Servie，而不用关注 database 的 Pod 可能被移动。

### 创建 Service

为了让 Service 选择多个运行相同应用的 Pod，我们使用 label selector。如下图，Service 选择管理三个具有 app=kubia 标签的 Pods。

![img](/img/post/post_deployment_1.png)

我们通过以下的描述文件创建一个 Service。**spec.selector** 选择要管理哪些 Pod，**Port** 指定了 Service 向外暴露的端口，**targetPort** 指定了 Service 监听对应容器的端口。

```yml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:   
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

### 连接集群之外的 Service



### 向外部 client 暴露 Service

以上我们介绍的是 Service 如何被集群内部的 Pod 消费。接下来我们介绍如何暴露 Service 使得集群外部的 client 可以访问。

![img](/img/post/post_deployment_1.png)

#### NodePort Service

向外部 client 暴露 Service 的第一种方法是使用 NodePort 类型的 Service。顾名思义，NodePort 类型的 Service 使得 Kubernetes 在 Service 管理的所有 Node 上预留一个端口，并将访问这个端口的外部请求转发给 Service，Service 再将请求分配给某个 Pod。

我们可以通过如下描述文件创建 Serivce。Service 的类型指定为 NodePort，nodePort 的端口指定为 30123。

```yml
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport 
spec:
  type: NodePort ports:
  - port: 80
    targetPort: 8080
    nodePort: 30123
  selector:
    app: kubia
```

我们可以观察创建的 Service。在 EXTERNAL-IP 一栏中显示 \<nodes\>，表示可以通过任意一个 Node 的 IP 来访问。PORT 栏显示了 cluster IP 的端口 80 和 Node Port 30123。所以 Service 可以通过以下几个地址访问：10.11.254.223:80 / <node’sIP>:30123。

```
$ kubectl get svc kubia-nodeport
NAME            CLUSTER-IP      EXTERNAL-IP     PORT(S)         AGE
kubia-nodeport  10.111.254.223  <nodes>         80:30123/TCP    2m
```

下图展示了一个外部请求被重定向到随机选择的一个 Pod 的过程。

![img](/img/post/post_deployment_1.png)

当使用 Minikube 时，可以使用如下命令访问 NodePort 类型的 Service。
```
minikube service <service-name> [-n <namespace>]
```

#### Load Balancer Service

一般云服务商提供的 Kubernetes 集群会自动提供 Load Balancer，Load Balancer 有自己独立的、可公开访问的 IP 地址，它会重定向所有到达 Service 的连接。如果 Kubernetes 运行的环境不支持 Load Balancer，它会自动转换为 NodePort 类型的 Service。这是因为 Load Balancer 其实是 Node Port 的扩展。要注意的是，Minkube 不支持 Load Balancer。

我们可以通过如下方式来创建一个 Load Balancer 类型的 Service。除了指定类型为 Load Balancer 以外没啥特别的。

```yml
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer 
spec:
  type: LoadBalancer ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

当利用 **kubectl create**命令创建后，云服务商会创建一个 Load Balancer，并把它的 IP 地址写入我们创建的 Service。然后我们查看创建的 Service，可以看到 EXTERNAL-IP 栏出现一个 IP 地址，我们就可以通过它访问 Service。如果显示 Pending 状态，表示正在分配 IP 地址，等一会儿再刷新看看。

```
$ kubectl get svc kubia-loadbalancer
NAME                CLUSTER-IP      EXTERNAL-IP       PORT(S)       AGE
kubia-loadbalancer  10.111.241.153  130.211.53.173    80:32143/TCP  1m
```

下图展示了 HTTP 请求如何被转发到 Pod 的过程。访问 Load Balancer 的 HTTP 请求首先被转发到 Node 分配的 Port 上，Node Port将请求转发给 Service，然后由 Service 路由到某一个 Pod 实例。

![img](/img/post/post_deployment_1.png)

#### Ingress 资源

除了以上两种方法向外暴露 Service 给 client，我们还可以通过创建一个 Ingress 资源向外暴露 Service。每一个 Load Balancer Service 都需要自己的一个 Load Balancer，而一个 Ingress 提供多个 Service 的访问，只需要一个 Load Balancer。当一个请求发送到 Ingress，Host 和 Path 决定了它应该被转发到哪一个 Service。

![img](/img/post/post_deployment_1.png)

Ingress 资源定义了上图中的各种路由规则，具体的规则定义请参考这一篇[文章]()。但要想让 Ingress 中的路由规则工作，我们需要运行一个 Ingress Controller，不同的 Kubernetes 环境提供的 Ingress Controller 各不相同，一般我们用官方维护的 [Nginx]()。下图展示了 Ingress Controller 的工作流程。

![img](/img/post/post_deployment_1.png)

### Readiness 探针

一旦有合适 Label 的新 Pod 被创建，它就会成为某个 Service 的一部分，被重定向一些请求。但如果新创建的 Pod 由于创建时间比较长还没准备好开始想听请求呢？这该怎么办。

之前在介绍 ReplicationController 时我们学习了Liveness 探针，它被用来检测一个 Pod 是否健康，不健康的 Pod 会自动重启。这里我们引入另一个探针 Readiness，它会周期性的调用来检测 Pod 是否可以正常响应请求，当 Readiness 探针返回成功时，代表该 Pod 准备好接受外部请求了。

不同于 Liveness 探针，Readiness 探针失败时不会重启 Pod，它只会确保 Service 包含能正常响应请求的 Pod。如下图所示，当一个 Pod 的 Readiness 探针失败了，它就会被移出 Endpoint 对象，到达这个 Service 的客户端连接将不再会路由到这个 Pod。

![img](/img/post/post_deployment_1.png)

#### Create Readiness Probe

Readiness 与 Liveness 探针相似，都是在 spec.template.spec.containers 下创建。如下例所示，Readiness 探针会周期性的在容器中执行 ls /var/ready 命令，如果文件存在，命令返回码为0，表示成功，否则返回失败。

```yml
apiVersion: v1
kind: ReplicationController 
...
spec:
...
  template: 
  ...
  spec: 
    containers:
    - name: kubia
      image: luksa/kubia
      readinessProbe:
        exec:
          command:
          - ls
          - /var/ready
      ...
```

### 

参考自：
1. Kuberneter in Action by Marko Luksa.

